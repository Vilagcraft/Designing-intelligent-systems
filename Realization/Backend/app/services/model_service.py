# app/services/model_service.py
import json
from pathlib import Path
from typing import Optional

# torch –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –æ–Ω –Ω—É–∂–µ–Ω (–ø–æ–∑–∂–µ) ‚Äî —É—Å–∫–æ—Ä—è–µ—Ç –∏–º–ø–∞—Ä—Ç –º–æ–¥—É–ª—è —Å–µ—Ä–≤–µ—Ä–∞
import importlib

from app.config import MODEL_PATH, VOCAB_PATH, CONFIG_PATH
from app.services.utils_service import load_nn_config, tokenize


class ModelService:

    def __init__(self):
        self.model = None
        self.vocab = None
        self.label2id = None
        self.id2label = None
        self.config = load_nn_config(CONFIG_PATH)
        self._torch = None
        # –≥—Ä—É–∑–∏–º —Å–ª–æ–≤–∞—Ä—å —Å—Ä–∞–∑—É (–µ—Å–ª–∏ –µ—Å—Ç—å), –º–æ–¥–µ–ª—å ‚Äî –ª–µ–Ω–∏–≤–æ
        self._load_vocab()
        self._prepare_labels()

    def _import_torch(self):
        if self._torch is None:
            try:
                import torch as _torch
            except Exception as e:
                raise RuntimeError("Torch –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ torch –≤ venv. –û—à–∏–±–∫–∞: " + str(e))
            self._torch = _torch
        return self._torch

    def _load_vocab(self):
        if not Path(VOCAB_PATH).exists():
            self.vocab = None
            return
        with open(VOCAB_PATH, "r", encoding="utf-8") as f:
            self.vocab = json.load(f)

    def _prepare_labels(self):
        labels = self.config.get("labels", [])
        self.label2id = {label: i for i, label in enumerate(labels)}
        self.id2label = {i: label for label, i in self.label2id.items()}

    def _ensure_model_loaded(self):
        """–õ–µ–Ω–∏–≤–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å (torch + –∫–ª–∞—Å—Å –º–æ–¥–µ–ª–∏)"""
        if self.model is not None:
            return

        # –ø—Ä–æ–≤–µp–∫–∞ vocab
        if self.vocab is None:
            raise RuntimeError(f"Vocab –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {VOCAB_PATH}. –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–π—Ç–µ {VOCAB_PATH}")

        torch = self._import_torch()

        # –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∏–º–ø–æ—Ä—Ç –∫–ª–∞—Å—Å–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–∞–∫–µ—Ç–∞ app.nn.src.model
        try:
            mod = importlib.import_module("app.nn.src.model")
            BiLSTMAttention = getattr(mod, "BiLSTMAttention")
        except Exception as e:
            raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–ª–∞—Å—Å BiLSTMAttention –∏–∑ app.nn.src.model: " + str(e))

        # —Å–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å –ø–æ –∫–æ–Ω—Ñ–∏–≥—É
        self.model = BiLSTMAttention(
            vocab_size=len(self.vocab),
            embedding_dim=self.config["model"]["embedding_dim"],
            hidden_dim=self.config["model"]["hidden_dim"],
            num_layers=self.config["model"]["num_layers"],
            dropout=self.config["model"]["dropout"],
            num_classes=len(self.label2id)
        )

        # –∑–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤
        if not Path(MODEL_PATH).exists():
            # –Ω–µ—Ç –º–æ–¥–µ–ª–∏ ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º model –≤ –ø–∞–º—è—Ç–∏, –Ω–æ –±–µ–∑ –≤–µ—Å–æ–≤
            print(f"‚ö† –ú–æ–¥–µ–ª—å –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ –ø—É—Ç–∏ {MODEL_PATH}. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω—ã –¥–æ –æ–±—É—á–µ–Ω–∏—è.")
            return

        try:
            # –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ weights_only=True (–±–µ–∑–æ–ø–∞—Å–Ω–µ–µ) ‚Äî –¥–æ—Å—Ç—É–ø–Ω–æ –≤ –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏—è—Ö torch
            try:
                state = torch.load(str(MODEL_PATH), map_location="cpu", weights_only=True)
                self.model.load_state_dict(state)
            except TypeError:
                # —Å—Ç–∞—Ä—ã–π torch ‚Äî –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ weights_only
                state = torch.load(str(MODEL_PATH), map_location="cpu")
                self.model.load_state_dict(state)
        except Exception as e:
            raise RuntimeError("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏: " + str(e))

        self.model.eval()
        print("üîß –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º.")

    def predict(self, text: str) -> dict:
        """
        –°–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è —Ç–µ–∫—Å—Ç–∞.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: {"label": "...", "logits": [...], "ok": True/False, "error": "..."}
        """
        try:
            # –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–ª–æ–≤–∞—Ä—å –∏ –º–æ–¥–µ–ª—å
            if self.vocab is None:
                return {"ok": False, "error": f"Vocab –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ {VOCAB_PATH}"}

            self._ensure_model_loaded()
            if self.model is None:
                return {"ok": False, "error": f"–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ {MODEL_PATH}"}

            # —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            tokens = tokenize(text)
            token_ids = [self.vocab.get(t, self.vocab.get("<unk>", 0)) for t in tokens]

            max_len = int(self.config["training"]["max_len"])
            token_ids = token_ids[:max_len]
            token_ids += [self.vocab.get("<pad>", 0)] * (max_len - len(token_ids))

            torch = self._import_torch()
            x = torch.tensor([token_ids], dtype=torch.long)

            with torch.no_grad():
                logits = self.model(x)
                probs = torch.softmax(logits, dim=1).squeeze(0).cpu().tolist()
                cls = int(logits.argmax(dim=1).item())
                label = self.id2label.get(cls, "unknown")

            return {"ok": True, "label": label, "probs": probs}
        except Exception as e:
            return {"ok": False, "error": str(e)}


# Singleton/–≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä (—Å–æ–∑–¥–∞—ë—Ç—Å—è –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è)
loaded_model = ModelService()